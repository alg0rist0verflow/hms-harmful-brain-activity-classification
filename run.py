# global
import os
os.environ["KERAS_BACKEND"] = "jax"
import keras_cv
import keras
from keras import ops
import tensorflow as tf
import cv2
import pandas as pd
import numpy as np
from glob import glob
from tqdm.notebook import tqdm
import joblib
import matplotlib.pyplot as plt 
import pywt
import librosa
# local
from config import CFG
from build_spectogram_tooling import build_timecourse_specs
from parquet2numpy_tooling import parquet2numpy, parquetEEG2NPY

print("TensorFlow:", tf.__version__)
print("Keras:", keras.__version__)
print("KerasCV:", keras_cv.__version__)
# make it reproducible
keras.utils.set_random_seed(CFG.seed)


directory_path = '{}/EEG_Spectrograms/'.format(CFG.DATA)
if not os.path.exists(directory_path):
    os.makedirs(directory_path)

os.makedirs(CFG.SPEC_DIR+'/train_spectrograms', exist_ok=True)
os.makedirs(CFG.SPEC_DIR+'/test_spectrograms', exist_ok=True)
os.makedirs(CFG.SPEC_DIR+'/train_eegs', exist_ok=True)
os.makedirs(CFG.SPEC_DIR+'/test_eegs', exist_ok=True)

# Train + Valid
df = pd.read_csv(f'{CFG.BASE_PATH}/train.csv')
df['eeg_path'] = f'{CFG.BASE_PATH}/train_eegs/'+df['eeg_id'].astype(str)+'.parquet'
df['spec_path'] = f'{CFG.BASE_PATH}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.parquet'
df['spec2_path'] = f'{CFG.SPEC_DIR}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.npy'
df['class_name'] = df.expert_consensus.copy()
df['class_label'] = df.expert_consensus.map(CFG.name2label)

# Test
test_df = pd.read_csv(f'{CFG.BASE_PATH}/test.csv')
test_df['eeg_path'] = f'{CFG.BASE_PATH}/test_eegs/'+test_df['eeg_id'].astype(str)+'.parquet'
test_df['spec_path'] = f'{CFG.BASE_PATH}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.parquet'
test_df['spec2_path'] = f'{CFG.SPEC_DIR}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.npy'

RUN_BUILD = False
RUN_PARQUET_CONVERT = False
if RUN_BUILD:
    build_timecourse_specs(df, directory_path)

if RUN_PARQUET_CONVERT:
    parquet2numpy(df, test_df, CFG)
    parquetEEG2NPY(df, test_df, CFG)

"""
# ðŸ”ª | Data Split

In the following code snippet, the data is divided into `5` folds. Note that, the `groups` argument is used to prevent any overlap of patients between the training and validation sets, thus avoiding potential **data leakage** issues. Additionally, each split is stratified based on the `class_label`, ensuring a uniform distribution of class labels in each fold.
"""
from sklearn.model_selection import StratifiedGroupKFold

sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)

df["fold"] = -1
df.reset_index(drop=True, inplace=True)
for fold, (train_idx, valid_idx) in enumerate(
    sgkf.split(df, y=df["class_label"], groups=df["patient_id"])
):
    df.loc[valid_idx, "fold"] = fold
df.groupby(["fold", "class_name"])[["eeg_id"]].count().T

"""
## Build Train & Valid Dataset

Only first sample for each `spectrogram_id` is used in order to keep the dataset size managable. Feel free to train on full data.
"""
# Sample from full data
from dataloader import build_dataset

sample_df = df.groupby("spectrogram_id").head(1).reset_index(drop=True)
train_df = sample_df[sample_df.fold != CFG.fold]
valid_df = sample_df[sample_df.fold == CFG.fold]
print(f"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}")

# Train
train_paths = train_df.spec2_path.values
train_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)
train_labels = train_df.class_label.values
train_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,
                         repeat=True, shuffle=True, augment=True, cache=True)

# Valid
valid_paths = valid_df.spec2_path.values
valid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)
valid_labels = valid_df.class_label.values
valid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,
                         repeat=False, shuffle=False, augment=False, cache=True)

"""
## Dataset Check

Let's visualize some samples from the dataset.
"""
imgs, tars = next(iter(train_ds))

num_imgs = 8
plt.figure(figsize=(4*4, num_imgs//4*5))
for i in range(num_imgs):
    plt.subplot(num_imgs//4, 4, i + 1)
    img = imgs[i].numpy()[...,0]  # Adjust as per your image data format
    img -= img.min()
    img /= img.max() + 1e-4
    tar = CFG.label2name[np.argmax(tars[i].numpy())]
    plt.imshow(img)
    plt.title(f"Target: {tar}")
    plt.axis('off')
    
plt.tight_layout()
plt.show()

"""
# ðŸ¤– | Modeling

This notebook uses the `EfficientNetV2 B2` from KerasCV's collection of pretrained models. To explore other models, simply modify the `preset` in the `CFG` (config). Check the [KerasCV website](https://keras.io/api/keras_cv/models/tasks/image_classifier/) for a list of available pretrained models.
"""
# Build Classifier
model = keras_cv.models.ImageClassifier.from_preset(
    CFG.preset, num_classes=CFG.num_classes
)

# Compile the model  
model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),
              loss=CFG.LOSS())

# Model Sumamry
model.summary()

"""
# âš“ | LR Schedule

A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.
"""
from dataloader import get_lr_callback
lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)
ckpt_cb = keras.callbacks.ModelCheckpoint("best_model.keras",
                                         monitor='val_loss',
                                         save_best_only=True,
                                         save_weights_only=False,
                                         mode='min')

"""
# ðŸš‚ | Training
"""
history = model.fit(
    train_ds, 
    epochs=CFG.epochs,
    callbacks=[lr_cb, ckpt_cb], 
    steps_per_epoch=len(train_df)//CFG.batch_size,
    validation_data=valid_ds, 
    verbose=CFG.verbose
)